{"cells":[{"cell_type":"markdown","metadata":{"id":"4HV6WFSjivwK"},"source":["# Content Based approach"]},{"cell_type":"markdown","metadata":{"id":"Q4Y5yuK_ivwK"},"source":["### Pre-Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7V_Y8XCivwK"},"outputs":[],"source":["from datasets import load_dataset\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmCZ5q5uivwL"},"outputs":[],"source":["dataset_meta = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Video_Games\", split=\"full\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qLMpbH-ivwL"},"outputs":[],"source":["temp_df_meta = pd.DataFrame(dataset_meta)\n","df_meta = temp_df_meta[['title','description','parent_asin', 'rating_number']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGedL1T1ivwM"},"outputs":[],"source":["df_meta_filtered = df_meta[df_meta['rating_number'] > 10]\n","df_meta_filtered = df_meta_filtered[df_meta_filtered['description'].apply(lambda x: len(x) >15)]\n","df_meta_filtered = df_meta_filtered.reset_index(drop=True)\n","\n","\n","\n","print(f\"Numero totale di prodotti prima dell'applicazione dei filtri: {len(df_meta):>10}\")\n","print(f\"Numero totale di prodotti dopo l'applicazione dei filtri: {len(df_meta_filtered):>11}\")\n","df_meta_filtered"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["min_reviews_per_user = 30\n","\n","df_filtring_meta = df.drop_duplicates()\n","\n","df_filtring_meta = df_filtring_meta[df_filtring_meta['verified_purchase'] == True]\n","\n","user_review_counts = df_filtring_meta['user_id'].value_counts()\n","users_with_min_reviews = user_review_counts[user_review_counts >= min_reviews_per_user].index\n","filtered_df_meta_avan = df[df['user_id'].isin(users_with_min_reviews)]\n","item_review_counts = filtered_df_meta_avan.groupby('parent_asin')['user_id'].nunique()\n","filtered_df_meta_avan = filtered_df_meta_avan[filtered_df_meta_avan['verified_purchase'] == True]\n","num_products = filtered_df_meta_avan['parent_asin'].nunique()\n","num_users = filtered_df_meta_avan['user_id'].nunique()\n","num_reviews = len(filtered_df_meta_avan)\n","\n","print(f'Numero di prodotti: {num_products}')\n","print(f'Numero di utenti: {num_users}')\n","print(f'Numero di recensioni totali: {num_reviews}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_df = filtered_df_meta_avan[filtered_df_meta_avan['parent_asin'].isin(df_meta_filtered['parent_asin'])]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_meta_filtered"]},{"cell_type":"markdown","metadata":{"id":"DHYaKKidivwN"},"source":["## Processing text column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ufF9V2yivwP"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y99QZ8rJivwP","outputId":"3c4cef7d-7e4a-4ce7-9410-dec4aae81c87"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def list_to_str(lst):\n","    return str(lst)\n","\n","df_meta_filtered['description'] = df_meta_filtered['description'].apply(list_to_str)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lemmatizer = WordNetLemmatizer() # meglio dello stemmer\n","stop_words = set(stopwords.words(\"english\"))\n","def preprocess_text(text):\n","    if isinstance(text, str):\n","        tokens = word_tokenize(text.lower())\n","        tokens = [word for word in tokens if word.isalnum()]\n","        tokens = [word for word in tokens if word not in stop_words]\n","        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","        return ' '.join(tokens)\n","    else:\n","        return ''\n","\n","# ho tolto le colonne title e description rating_number, helpful_vote, verified_purchase e lasciato solo quelle processate\n","df_meta_filtered['text'] = (df_meta_filtered['title'] + ' ' + df_meta_filtered[\"description\"]).apply(preprocess_text)\n","df_meta_filtered.drop_duplicates()\n","df_meta_filtered.sample(1)"]},{"cell_type":"markdown","metadata":{"id":"KjC4MkMmivwR"},"source":["## Text Embedding - BoW Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from collections import defaultdict\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7Q5hxtpivwS"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words(\"english\"))\n","punctuation = set(string.punctuation)\n","\n","vocab = set()\n","bow_model = []\n","raw_text = df_meta_filtered[\"text\"]\n","for text in (raw_text):\n","    word_counts = defaultdict(int)\n","    tokens = word_tokenize(text.lower())\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n","    \n","    vocab.update(tokens)\n","    for word in tokens:\n","        word_counts[word] += 1\n","    \n","    bow_model.append(word_counts)\n","\n","vocab = list(vocab)\n","print(f\"Numero di parole nel vocabolario: {len(vocab)}\")\n","print(f\"Le 10 parole pi√π frequenti nel primo documento: {sorted(vocab, key=lambda x: bow_model[0].get(x, 0), reverse=True)[:10]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytqpQsZNivwU","outputId":"39a1c6e4-60e0-4740-9f0d-442535ad1e9a"},"outputs":[],"source":["bow_data = pd.DataFrame(0, index=range(len(raw_text)), columns=list(vocab))\n","for i in range(len(df_meta_filtered['text'])):\n","  bow_data.loc[i, bow_model[i].keys()] = bow_model[i].values()\n","bow_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizer = CountVectorizer()\n","bow_model = vectorizer.fit_transform(df_meta_filtered['text'])\n","bow_dataset = pd.DataFrame(bow_model.toarray(), columns=vectorizer.get_feature_names_out())\n","bow_dataset[\"parent_asin\"] = df_meta_filtered[\"parent_asin\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bow_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["user_id = 'AHLK5V5OBWUPTZZMJ2XIKBR4LUHA'\n","print(f'User: {user_id}')\n","user_ratings = filtered_df[filtered_df['user_id'] == user_id]\n","rated_items = bow_dataset[bow_dataset['parent_asin'].isin(user_ratings['parent_asin'])]\n","print(f'# rated items: {len(rated_items)}')\n","dataset = pd.merge(rated_items, user_ratings, on=\"parent_asin\")\n","dataset = dataset.drop(columns=[\"parent_asin\", \"user_id\", \"verified_purchase\", \"title_y\", \"text_y\"])\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=\"rating_y\"),\n","                                                    dataset['rating_y'],\n","                                                    test_size=0.20,\n","                                                    random_state=0)\n","neigh_reg = KNeighborsRegressor(n_neighbors=10, metric=\"cosine\")\n","neigh_reg.fit(X_train, y_train)\n","y_pred = neigh_reg.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","print(f'MSE = {mse:.6f}')\n","print(f'RMSE = {rmse:.6f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mse_users = []\n","for user_id in filtered_df[\"user_id\"].unique():\n","  user_ratings = filtered_df[filtered_df['user_id'] == user_id]\n","  rated_items = bow_dataset[bow_dataset['parent_asin'].isin(user_ratings['parent_asin'])]\n","  dataset = pd.merge(rated_items, user_ratings, on=\"parent_asin\")\n","  dataset = dataset.drop(columns=[\"parent_asin\", \"user_id\", \"verified_purchase\", \"title_y\", \"text_y\"])\n","  try:\n","    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=\"rating_y\"),\n","                                                        dataset['rating_y'],\n","                                                        test_size=0.20,\n","                                                        random_state=0)\n","    neigh_reg = KNeighborsRegressor(n_neighbors=min(20, len(X_train)),\n","                                    metric=\"cosine\")\n","    neigh_reg.fit(X_train, y_train)\n","    y_pred = neigh_reg.predict(X_test)\n","    mse = mean_squared_error(y_test, y_pred)\n","    mse_users.append(mse)\n","  except:\n","    continue"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Average MSE over users: {np.mean(mse_users):.2f}\")\n","print(f\"Average RMSE over users: {np.sqrt(np.mean(mse_users)):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"9P5ia8MuivwZ"},"source":["## Text Embedding - Transformers Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88HSPV-3ivwa"},"outputs":[],"source":["import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","from sentence_transformers import SentenceTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vro3GACivwa"},"outputs":[],"source":["model = SentenceTransformer('sentence-transformers/average_word_embeddings_komninos')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"af8APIfUivwb"},"outputs":[],"source":["embeddings = model.encode(df_meta_filtered[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_meta_filtered"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embeddings_dataset = pd.DataFrame(embeddings)\n","embeddings_dataset[\"parent_asin\"] = df_meta_filtered[\"parent_asin\"]\n","embeddings_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mse_users = []\n","for user_id in filtered_df[\"user_id\"].unique():\n","    user_ratings = filtered_df[filtered_df['user_id'] == user_id]\n","    rated_items = embeddings_dataset[embeddings_dataset['parent_asin'].isin(user_ratings['parent_asin'])]\n","    dataset_rec = pd.merge(rated_items, user_ratings, on=\"parent_asin\")\n","    dataset_rec = dataset_rec.drop(columns=[\"parent_asin\", \"user_id\"])\n","    dataset_rec = pd.get_dummies(dataset_rec, columns=dataset_rec.select_dtypes(include=['object']).columns)\n","    dataset_rec = dataset_rec.dropna()\n","    dataset_rec.columns = dataset_rec.columns.astype(str)\n","    if len(dataset_rec) == 0 or 'rating' not in dataset_rec.columns:\n","        continue\n","    try:\n","        X_train, X_test, y_train, y_test = train_test_split(dataset_rec.drop(columns=\"rating\"),\n","                                                            dataset_rec['rating'],\n","                                                            test_size=0.20,\n","                                                            random_state=0)\n","        if len(X_train) < 2:\n","            continue\n","        neigh_reg = KNeighborsRegressor(n_neighbors=min(40, len(X_train)), metric=\"cosine\")\n","        neigh_reg.fit(X_train, y_train)\n","        y_pred = neigh_reg.predict(X_test)\n","        mse = mean_squared_error(y_test, y_pred)\n","        mse_users.append(mse)\n","    except Exception as e:\n","        print(f'Error for user {user_id}: {e}')\n","        continue\n","\n","if mse_users:\n","    average_mse = np.mean(mse_users)\n","    print(f'Average MSE: {average_mse:.6f}')\n","else:\n","    print('No MSE values calculated.')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Average MSE over users: {np.mean(mse_users):.2f}\")\n","print(f\"Average RMSE over users: {np.sqrt(np.mean(mse_users)):.2f}\")"]}],"metadata":{"colab":{"collapsed_sections":["zIBZJ2J8ivv1","WaMD_JDBmizw","xSVz9hxgmsEd","pWvPdZ_hm9Zy","VqC36q_Togrk","nIEMubZ9p5fe","HmHjv2WdpyfG","iG3U7qoRN_DV","lzqBH2UQM_K9","TBrTQ2M6ivv4","pcAlMClOeLK7","uMhKTtiAeOQw","aNilevZQivv_","Ri5pflGlivwH","uNBUosWZaUFf","3o6TFizgdbyN","4d_g3MgJivwJ","4HV6WFSjivwK"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
